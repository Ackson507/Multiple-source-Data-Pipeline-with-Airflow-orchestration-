# Comming SOON!
The Steps in this project will be summarized because there are the same as earlier completed project where we created a pipeline with a single source of data. if you have not checked it, do so on the below link.
- Linear  Data Pipeline with Airflow orchestration
![636e2e99d3a9546506082364_Data Pipeline Components](https://github.com/Ackson507/Web-Scraping-Data-Pipeline/assets/84422970/64ba5334-dac7-4b85-8ba9-b0d9d7ee1805)

### Objective
Building a data pipeline from multiple sources into Postgres Database for storage purposes.

### Identify Data Sources
Recognizing your data sources is a crucial step in building a data pipeline. Data sources for this project will include;
use include  be broadly divided into six categories:
- Source 1: Databases which is PostgreSQL will need some data from certain database.
- Sources 2: CSV files.
- Source 3: APIs (Application Programming Interfaces): Data can be extracted from APIs that provide a direct connection to various web services and external platforms.”

![Batch-processing-pipeline](https://github.com/Ackson507/Multiple-source-Data-Pipeline-with-Airflow-orchestration-/assets/84422970/8cb07e11-560a-4a2c-ba54-40a09484e267)


### Determine the Data Ingestion Strategy
- Batch Ingestion will be used as data will be collected over a specific period such as once in a week or month and processing it as a group. 

### Design the Data Processing Plan.
The plan to be used here is ETL where will extract data from the source then clean, transform it, and then load it into the destination. 

### Decide Where to Store the Information
Our destination storage is database system PostgresSQL stored in a clean and structured format

### Initiating the Workflow on our Local Machine.
The sequence of operations in the pipeline includes specifying the order of tasks, managing dependencies between tasks, handling errors, and setting up retries or notifications in case of failure will be done by Apache Airflow

### Set a Monitoring Framework
- Once your pipeline is operational, monitoring its performance to ensure it’s working as expected will be done by Apache Airflow webserver.

### Implement Data Consumption Layer
Finally, will use business intelligence (BI) tools to create and build visual interface through which end-users or applications access the processed data in form od dashboard. 

End of the project, Lookout as it will be completed soon for the sake of learning and practice.

