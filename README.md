# Web-Scraping-Data-Pipeline
Build a data pipeline that scrapes data from a website, processes it, and stores in database ready for analysis. A data pipeline is a series of processes and tools that automate the movement, transformation, and management of data from various sources to a destination, typically for the purpose of analysis, storage, or further processing.
![iv6RhmW](https://github.com/Ackson507/Web-Scraping-Data-Pipeline/assets/84422970/3e8e211b-d29e-46ca-b234-f0cb96b7c12f)
Below is our roadmap and will involve the following tasks;
- Data Source: API or webpages
- Data collection: BeautifulSoup or Scrapy (for web scraping)
- Data Processing: Using Pyspark or Pandas for data cleaning, processing and transformation
- Destination: PostgreSQL (for data storage)
- Data Orchestration: Workflow management and monitoring tools like Apache Airflow or proprietary services that schedule and monitor the execution of various stages in the data pipeline. This will automation data flows ensures that data is consistently and accurately processed without manual intervention.

![636e2e99d3a9546506082364_Data Pipeline Components](https://github.com/Ackson507/Web-Scraping-Data-Pipeline/assets/84422970/64ba5334-dac7-4b85-8ba9-b0d9d7ee1805)

### Data Source: API or webpages
SBT is one of the leading automobile trading companies whose headquarter is located in Yokohama, Japan.
- url = https://www.sbtjapan.com/used-cars/

### Data collection: BeautifulSoup or Scrapy (for web scraping)


### Data Processing: Using Pyspark or Pandas for data cleaning, processing and transformation


### Destination: PostgreSQL (for data storage)


### Data Orchestration
